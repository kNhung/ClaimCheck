import os
from functools import lru_cache
from threading import Lock

from sentence_transformers import SentenceTransformer, CrossEncoder
import numpy as np
import nltk
# nltk.download('punkt')
from nltk.tokenize import sent_tokenize
import requests
from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning

# dotenv.load_dotenv() is called in factchecker/__init__.py

_EMBED_DEVICE = os.getenv("FACTCHECKER_EMBED_DEVICE")
_BI_MODEL_NAME = os.getenv("FACTCHECKER_BI_ENCODER", "paraphrase-multilingual-MiniLM-L12-v2")
_CROSS_MODEL_NAME = os.getenv("FACTCHECKER_CROSS_ENCODER", "cross-encoder/ms-marco-MiniLM-L-6-v2")

# Global model cache with thread-safe initialization
_bi_model_cache = None
_cross_model_cache = None
_model_lock = Lock()

def scrape_text(url):
    try:
        resp = requests.get(url, timeout=5)
        soup = BeautifulSoup(resp.text, "html.parser")
        paragraphs = soup.find_all('p')
        text = " ".join([p.get_text() for p in paragraphs])
        # Clear soup object to free memory
        del soup
        return text
    except:
        return ""

def chunk_text(text, chunk_size=60):
    """
    Chunk text th√†nh c√°c ƒëo·∫°n nh·ªè.
    TƒÉng chunk_size t·ª´ 50 l√™n 60 ƒë·ªÉ gi·∫£m s·ªë chunks, tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω.
    """
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    for sent in sentences:
        current_chunk += " " + sent
        if len(current_chunk.split()) >= chunk_size:
            chunks.append(current_chunk.strip())
            current_chunk = ""
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

def _get_bi_model(model_name=_BI_MODEL_NAME):
    """Get bi-encoder model with thread-safe caching."""
    global _bi_model_cache
    if _bi_model_cache is None:
        with _model_lock:
            # Double-check pattern to avoid race condition
            if _bi_model_cache is None:
                kwargs = {}
                if _EMBED_DEVICE:
                    kwargs["device"] = _EMBED_DEVICE
                _bi_model_cache = SentenceTransformer(model_name, **kwargs)
    return _bi_model_cache


def _get_cross_model(model_name=_CROSS_MODEL_NAME):
    """Get cross-encoder model with thread-safe caching."""
    global _cross_model_cache
    if _cross_model_cache is None:
        with _model_lock:
            # Double-check pattern to avoid race condition
            if _cross_model_cache is None:
                kwargs = {}
                if _EMBED_DEVICE:
                    kwargs["device"] = _EMBED_DEVICE
                _cross_model_cache = CrossEncoder(model_name, **kwargs)
    return _cross_model_cache


def preload_models():
    """
    Pre-load models to avoid loading them multiple times in multi-threaded scenarios.
    This should be called once before starting parallel processing.
    """
    print("Pre-loading models to avoid multiple loads in threads...")
    try:
        _get_bi_model()
        print("‚úì Bi-encoder model pre-loaded")
    except Exception as e:
        print(f"Warning: Failed to pre-load bi-encoder model: {e}")
    
    try:
        _get_cross_model()
        print("‚úì Cross-encoder model pre-loaded")
    except Exception as e:
        print(f"Warning: Failed to pre-load cross-encoder model: {e}")


def get_top_evidence(claim, text, top_k_chunks=None, p=6, q=2, log_callback=None, return_score=False):
    """
    RAV (Retrieval-Augmented Verification) ƒë·ªÉ l·∫•y top evidence t·ª´ text.
    T·ªëi ∆∞u t·ªëc ƒë·ªô: gi·∫£m p t·ª´ 10 xu·ªëng 6 v√† q t·ª´ 3 xu·ªëng 2 ƒë·ªÉ x·ª≠ l√Ω √≠t chunks h∆°n v·ªõi CrossEncoder.
    
    Args:
        claim: C√¢u claim c·∫ßn fact-check
        text: Text c·∫ßn t√¨m evidence
        top_k_chunks: (Deprecated) Gi·ªØ ƒë·ªÉ backward compatibility. N·∫øu ƒë∆∞·ª£c set, d√πng cho c·∫£ p v√† q.
        p: S·ªë l∆∞·ª£ng top candidates t·ª´ bi-encoder (default: 6, gi·∫£m t·ª´ 10 ƒë·ªÉ tƒÉng t·ªëc)
        q: S·ªë l∆∞·ª£ng top candidates t·ª´ cross-encoder sau khi re-rank (default: 2, gi·∫£m t·ª´ 3 ƒë·ªÉ tƒÉng t·ªëc)
        log_callback: H√†m callback ƒë·ªÉ log c√°c b∆∞·ªõc (optional)
        return_score: N·∫øu True, tr·∫£ v·ªÅ tuple (summary, max_score) v·ªõi max_score l√† cross-encoder score cao nh·∫•t
    
    Returns:
        N·∫øu return_score=False:
            - N·∫øu q=1: str - best chunk
            - N·∫øu q>1: str - c√°c chunks ƒë∆∞·ª£c join l·∫°i
        N·∫øu return_score=True:
            - tuple: (summary_str, max_score_float)
    """
    if log_callback:
        log_callback("üîç B∆Ø·ªöC 1: Chunking text th√†nh c√°c ƒëo·∫°n nh·ªè")
    
    all_chunks = chunk_text(text)
    
    if log_callback:
        log_callback(f"   ‚Üí T·ªïng s·ªë chunks ƒë∆∞·ª£c t·∫°o: {len(all_chunks)}")
        if len(all_chunks) > 0:
            log_callback(f"   ‚Üí ƒê·ªô d√†i trung b√¨nh m·ªói chunk: {sum(len(c.split()) for c in all_chunks) / len(all_chunks):.1f} t·ª´")

    if not all_chunks:
        if log_callback:
            log_callback("   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y chunks n√†o!")
        if return_score:
            return "No evidence found.", 0.0
        return "No evidence found."
    
    # Backward compatibility: n·∫øu top_k_chunks ƒë∆∞·ª£c set, d√πng cho c·∫£ p v√† q
    if top_k_chunks is not None:
        p = top_k_chunks
        q = 1
    
    # ƒê·∫£m b·∫£o p >= q v√† p kh√¥ng v∆∞·ª£t qu√° s·ªë chunks
    p = min(p, len(all_chunks))
    q = min(q, p)
    
    if log_callback:
        log_callback(f"\nüîç B∆Ø·ªöC 2: Bi-encoder - L·∫•y top {p} candidates")
        log_callback(f"   ‚Üí S·ª≠ d·ª•ng model: {_BI_MODEL_NAME}")
    
    # Step 1: Bi-encoder - l·∫•y top p candidates
    bi_model = _get_bi_model()
    claim_emb = bi_model.encode(claim)
    chunk_embs = bi_model.encode(all_chunks)
    claim_emb /= np.linalg.norm(claim_emb)
    chunk_embs /= np.linalg.norm(chunk_embs, axis=1, keepdims=True)
    cos_sims = np.dot(chunk_embs, claim_emb)
    
    # L·∫•y top p indices
    top_p_indices = np.argsort(-cos_sims)[:p]
    top_p_chunks = [all_chunks[i] for i in top_p_indices]
    top_p_scores = [float(cos_sims[i]) for i in top_p_indices]
    
    if log_callback:
        log_callback(f"   ‚Üí Top {p} chunks t·ª´ bi-encoder (cosine similarity):")
        for idx, (chunk_idx, score) in enumerate(zip(top_p_indices, top_p_scores)):
            # Ghi ƒë·∫ßy ƒë·ªß chunk, kh√¥ng truncate
            log_callback(f"      [{idx+1}] Chunk #{chunk_idx} (score: {score:.4f}): {all_chunks[chunk_idx]}")
    
    if log_callback:
        log_callback(f"\nüîç B∆Ø·ªöC 3: Cross-encoder re-rank - L·∫•y top {q} t·ª´ {p} candidates")
        log_callback(f"   ‚Üí S·ª≠ d·ª•ng model: {_CROSS_MODEL_NAME}")
    
    # Step 2: Cross-encoder re-rank - l·∫•y top q t·ª´ p candidates
    cross_model = _get_cross_model()
    pairs = [[claim, ch] for ch in top_p_chunks]
    cross_scores = cross_model.predict(pairs)
    
    # L·∫•y top q t·ª´ cross-encoder scores
    top_q_indices = np.argsort(-cross_scores)[:q]
    top_q_chunks = [top_p_chunks[i] for i in top_q_indices]
    top_q_scores = [float(cross_scores[i]) for i in top_q_indices]
    
    if log_callback:
        log_callback(f"   ‚Üí Top {q} chunks sau khi re-rank (cross-encoder scores):")
        for idx, (orig_idx, score) in enumerate(zip(top_q_indices, top_q_scores)):
            # Ghi ƒë·∫ßy ƒë·ªß chunk, kh√¥ng truncate
            log_callback(f"      [{idx+1}] Chunk #{top_p_indices[orig_idx]} (score: {score:.4f}): {top_p_chunks[orig_idx]}")
    
    if log_callback:
        log_callback(f"\n‚úÖ K·∫æT QU·∫¢: ƒê√£ ch·ªçn {q} chunk(s) t·ª´ {len(all_chunks)} chunks ban ƒë·∫ßu")
    
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£
    if q == 1:
        summary = top_q_chunks[0]
    else:
        # Join c√°c chunks l·∫°i v·ªõi nhau
        summary = " ".join(top_q_chunks)
    
    if return_score:
        # T√≠nh relevance score: d√πng max bi-encoder score (cosine similarity) ƒë√£ normalize [0, 1]
        # Bi-encoder score ƒë√£ normalize: cosine similarity [-1, 1] -> [0, 1]
        max_bi_score = max(top_p_scores) if top_p_scores else 0.0
        # Cosine similarity ƒë√£ normalize v·ªÅ [0, 1] trong code tr√™n
        # Nh∆∞ng th·ª±c t·∫ø cos_sims l√† [-1, 1], c·∫ßn normalize th√™m
        max_relevance_score = (max_bi_score + 1.0) / 2.0  # Normalize t·ª´ [-1, 1] v·ªÅ [0, 1]
        
        if log_callback:
            log_callback(f"   ‚Üí Max relevance score (bi-encoder, normalized): {max_relevance_score:.4f}")
        
        return summary, max_relevance_score
    return summary

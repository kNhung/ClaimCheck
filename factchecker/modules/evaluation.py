"""
Graph-based evaluation module for fact verification.
Based on "Evidence Retrieval is almost All You Need for Fact Verification" 
and graph-based evidence aggregation approaches like GEAR.
"""

import re
import json
import numpy as np
from functools import lru_cache
from threading import Lock
from typing import List, Tuple, Dict
from sentence_transformers import SentenceTransformer, CrossEncoder
import os
from . import llm

# Use same models as retriver_rav for consistency
_EMBED_DEVICE_ENV = os.getenv("FACTCHECKER_EMBED_DEVICE")
_BI_MODEL_NAME = os.getenv("FACTCHECKER_BI_ENCODER", "paraphrase-multilingual-MiniLM-L12-v2")
_CROSS_MODEL_NAME = os.getenv("FACTCHECKER_CROSS_ENCODER", "cross-encoder/ms-marco-MiniLM-L-6-v2")

# Global model cache with thread-safe initialization
_bi_model_cache = None
_cross_model_cache = None
_model_lock = Lock()


def _get_safe_device():
    """
    Get device for model loading with automatic fallback to CPU if GPU is not available.
    
    Returns:
        str: Device string ('cuda', 'cpu', etc.) that is safe to use
    """
    device = _EMBED_DEVICE_ENV or "cpu"
    
    # If device is set to GPU-related values, check availability
    if device.lower() in ("cuda", "gpu"):
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
            else:
                print(f"âš ï¸  GPU requested but not available. Falling back to CPU.")
                return "cpu"
        except ImportError:
            print(f"âš ï¸  PyTorch not available. Falling back to CPU.")
            return "cpu"
    
    return device


def _get_bi_model(model_name=_BI_MODEL_NAME):
    """Get bi-encoder model with thread-safe caching."""
    global _bi_model_cache
    if _bi_model_cache is None:
        with _model_lock:
            # Double-check pattern to avoid race condition
            if _bi_model_cache is None:
                try:
                    device = _get_safe_device()
                    kwargs = {"device": device}
                    _bi_model_cache = SentenceTransformer(model_name, **kwargs)
                    print(f"âœ“ Loaded bi-encoder model on device: {device}")
                except Exception as e:
                    # Fallback to CPU if any error occurs
                    print(f"âš ï¸  Error loading model on {device}, falling back to CPU: {e}")
                    kwargs = {"device": "cpu"}
                    _bi_model_cache = SentenceTransformer(model_name, **kwargs)
                    print(f"âœ“ Loaded bi-encoder model on device: cpu (fallback)")
    return _bi_model_cache


def _get_cross_model(model_name=_CROSS_MODEL_NAME):
    """Get cross-encoder model with thread-safe caching."""
    global _cross_model_cache
    if _cross_model_cache is None:
        with _model_lock:
            # Double-check pattern to avoid race condition
            if _cross_model_cache is None:
                try:
                    device = _get_safe_device()
                    kwargs = {"device": device}
                    _cross_model_cache = CrossEncoder(model_name, **kwargs)
                    print(f"âœ“ Loaded cross-encoder model on device: {device}")
                except Exception as e:
                    # Fallback to CPU if any error occurs
                    print(f"âš ï¸  Error loading model on {device}, falling back to CPU: {e}")
                    kwargs = {"device": "cpu"}
                    _cross_model_cache = CrossEncoder(model_name, **kwargs)
                    print(f"âœ“ Loaded cross-encoder model on device: cpu (fallback)")
    return _cross_model_cache

def preload_models():
    """
    Pre-load models to avoid loading them multiple times in multi-threaded scenarios.
    This should be called once before starting parallel processing.
    """
    print("Pre-loading evaluation models to avoid multiple loads in threads...")
    try:
        _get_bi_model()
        print("âœ“ Evaluation bi-encoder model pre-loaded")
    except Exception as e:
        print(f"Warning: Failed to pre-load bi-encoder model: {e}")
    
    try:
        _get_cross_model()
        print("âœ“ Evaluation cross-encoder model pre-loaded")
    except Exception as e:
        print(f"Warning: Failed to pre-load cross-encoder model: {e}")

def extract_claim_from_record(record: str) -> str:
    """
    Extract the claim from the record.
    Only matches claim at the beginning of the record (first 100 lines) to avoid
    matching with "Claim:" that might appear in Action Needed or other sections.
    """
    # Only search in the first 100 lines to avoid matching with "Claim:" in Action Needed
    lines = record.strip().split('\n')
    first_part = '\n'.join(lines[:100])
    
    # Look for "# Claim: ..." pattern in the first part only
    match = re.search(r'^#\s*Claim:\s*(.+?)(?:\n##|\n###|$)', first_part, re.IGNORECASE | re.MULTILINE | re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # Fallback: first line if no pattern found
    if lines:
        first_line = lines[0].strip()
        # Remove markdown formatting
        first_line = first_line.replace('#', '').replace('Claim:', '').strip()
        if first_line:
            return first_line
    return ""

def extract_evidence_pieces(record: str) -> List[str]:
    evidence_pieces = []
    
    # Pattern 1: web_search('...'), Summary: ... (format hiá»‡n táº¡i)
    pattern1 = re.compile(r"web_search\([^)]+\)\s*,\s*Summary:\s*(.+?)(?=\n\n|\n###|$)", re.DOTALL | re.IGNORECASE)
    matches1 = pattern1.findall(record)
    evidence_pieces.extend([m.strip() for m in matches1 if m.strip()])
    
    # Pattern 2: web_search(...) summary: ... (format cÅ©, khÃ´ng cÃ³ dáº¥u pháº©y)
    pattern2 = re.compile(r"web_search\([^)]+\)\s+summary:\s*(.+?)(?=\n\n|\n###|$)", re.DOTALL | re.IGNORECASE)
    matches2 = pattern2.findall(record)
    evidence_pieces.extend([m.strip() for m in matches2 if m.strip()])
    
    # Pattern 3: Look for evidence section vá»›i cáº£ 2 formats
    # CHá»ˆ láº¥y actual evidence summaries, Bá» QUA log text vÃ  metadata
    evidence_section_match = re.search(r'###\s*Evidence\s*\n\n(.+?)(?=\n###|$)', record, re.DOTALL | re.IGNORECASE)
    if evidence_section_match:
        evidence_text = evidence_section_match.group(1)
        lines = [line.strip() for line in evidence_text.split('\n') if line.strip()]
        for line in lines:
            # Bá» qua log text vÃ  metadata
            if any(skip in line for skip in ['ğŸ“‹', 'ğŸ”', 'âœ…', 'â†’', 'â€¢', 'BÆ¯á»šC', 'WEB SEARCH', 'WEB SCRAPING', 'RAV', 'Chunk #', 'score:', 'Content preview:', 'Snippets preview:', 'URLs:', 'Query:', 'Domain:', 'Content length:', 'Reason:', 'Failed:', 'Output:', 'Input:']):
                continue
            # Xá»­ lÃ½ cáº£ Summary: vÃ  summary:
            if 'summary:' in line.lower() or 'Summary:' in line:
                parts = re.split(r'summary:\s*', line, 1, flags=re.IGNORECASE)
                if len(parts) > 1:
                    summary_text = parts[1].strip()
                    # Chá»‰ thÃªm náº¿u khÃ´ng pháº£i lÃ  log text
                    if not any(skip in summary_text for skip in ['ğŸ“‹', 'ğŸ”', 'âœ…', 'â†’', 'â€¢', 'BÆ¯á»šC']):
                        evidence_pieces.append(summary_text)
            elif not re.match(r'web_search\([^)]+\)', line, re.IGNORECASE):
                # Chá»‰ thÃªm náº¿u lÃ  actual evidence content (khÃ´ng pháº£i log)
                if len(line) > 20 and not any(skip in line for skip in ['ğŸ“‹', 'ğŸ”', 'âœ…', 'â†’', 'â€¢', 'BÆ¯á»šC']):
                    evidence_pieces.append(line)
    
    # Remove duplicates
    seen = set()
    unique_evidence = []
    for ev in evidence_pieces:
        ev_lower = ev.lower()
        if ev_lower not in seen and len(ev) > 10:
            seen.add(ev_lower)
            unique_evidence.append(ev)
    
    return unique_evidence

def compute_evidence_scores_bi_encoder(claim: str, evidence_pieces: List[str]) -> np.ndarray:
    """
    Compute claim-evidence similarity scores using bi-encoder (fast, approximate).
    This is used for pre-filtering before using the slower cross-encoder.
    
    Returns:
        Array of cosine similarity scores [0, 1] for each evidence piece
    """
    if not evidence_pieces:
        return np.array([])
    
    bi_model = _get_bi_model()
    claim_emb = bi_model.encode(claim, normalize_embeddings=True)
    evidence_embs = bi_model.encode(evidence_pieces, normalize_embeddings=True)
    
    # Compute cosine similarities (already normalized)
    cos_sims = np.dot(evidence_embs, claim_emb)
    
    # Normalize to [0, 1] range (cosine similarity is in [-1, 1])
    cos_sims = (cos_sims + 1.0) / 2.0
    
    return cos_sims

def compute_evidence_scores(claim: str, evidence_pieces: List[str]) -> np.ndarray:
    """
    Compute fine-grained claim-evidence alignment scores using cross-encoder.
    
    Returns:
        Array of scores for each evidence piece
    """
    if not evidence_pieces:
        return np.array([])
    
    cross_model = _get_cross_model()
    pairs = [[claim, ev] for ev in evidence_pieces]
    scores = cross_model.predict(pairs)
    return scores

def extract_verdict(conclusion):
    """
    Extract verdict from conclusion text (for compatibility).
    Uses simple regex matching instead of LLM.
    """
    # Look for verdict in backticks
    match = re.search(r'`([^`]+)`', conclusion)
    if match:
        verdict = match.group(1).strip()
        # Normalize to standard labels
        verdict_lower = verdict.lower()
        if 'support' in verdict_lower:
            return '`Supported`'
        elif 'refut' in verdict_lower or 'bÃ¡c bá»' in verdict_lower:
            return '`Refuted`'
        elif 'not enough' in verdict_lower or 'khÃ´ng Ä‘á»§' in verdict_lower:
            return '`Not Enough Evidence`'
        return f'`{verdict}`'
    
    # Look for verdict in markdown bold
    match = re.search(r'\*\*([^*]+)\*\*', conclusion)
    if match:
        return f'`{match.group(1).strip()}`'
    
    # Default
    return '`Not Enough Evidence`'

def _normalize_llm_verdict(raw: str) -> str:
    """
    Chuáº©n hÃ³a verdict tá»« LLM vá» 3 lá»›p chuáº©n.
    """
    if not raw:
        return "Not Enough Evidence"
    s = str(raw).strip().lower()
    if "support" in s or "Ä‘Ãºng" in s or "cÃ³ cÄƒn cá»©" in s or "Ä‘Æ°á»£c há»— trá»£" in s:
        return "Supported"
    if "refut" in s or "sai" in s or "bÃ¡c bá»" in s or "trÃ¡i sá»± tháº­t" in s:
        return "Refuted"
    if "not enough" in s or "khÃ´ng Ä‘á»§" in s or "chÆ°a Ä‘á»§" in s or "khÃ´ng cÃ³ Ä‘á»§" in s:
        return "Not Enough Evidence"
    # Fallback: náº¿u khÃ´ng khá»›p rÃµ, coi lÃ  Not Enough Evidence Ä‘á»ƒ an toÃ n
    return "Not Enough Evidence"


def filter_evidence_by_relevance(claim: str, evidence_pieces: List[str], 
                                  relevance_threshold: float = 0.3,
                                  min_keep: int = 3,
                                  bi_prefilter_top_n: int = 15,
                                  log_callback=None) -> Tuple[List[str], List[float]]:
    """
    Lá»c evidence dá»±a trÃªn relevance score vá»›i claim.
    Sá»­ dá»¥ng 2-stage filtering:
    1. Pre-filter vá»›i Bi-encoder (nhanh) Ä‘á»ƒ láº¥y top N evidence
    2. Fine-grained filtering vá»›i CrossEncoder (cháº­m nhÆ°ng chÃ­nh xÃ¡c) cho top N
    
    Chá»‰ giá»¯ láº¡i evidence cÃ³ relevance score > threshold.
    NHÆ¯NG luÃ´n Ä‘áº£m báº£o giá»¯ láº¡i Ã­t nháº¥t min_keep evidence (top evidence).
    
    Args:
        claim: Claim cáº§n fact-check
        evidence_pieces: Danh sÃ¡ch evidence pieces
        relevance_threshold: NgÆ°á»¡ng relevance tá»‘i thiá»ƒu (default: 0.3)
        min_keep: Sá»‘ lÆ°á»£ng evidence tá»‘i thiá»ƒu cáº§n giá»¯ láº¡i (default: 3)
        bi_prefilter_top_n: Sá»‘ lÆ°á»£ng evidence Ä‘á»ƒ pre-filter báº±ng Bi-encoder trÆ°á»›c khi dÃ¹ng CrossEncoder (default: 15)
        log_callback: HÃ m callback Ä‘á»ƒ log cÃ¡c bÆ°á»›c (optional)
    
    Returns:
        Tuple[List[str], List[float]]: (filtered_evidence, relevance_scores) - chá»‰ giá»¯ evidence liÃªn quan
    """
    if not evidence_pieces:
        if log_callback:
            log_callback("âš ï¸ KhÃ´ng cÃ³ evidence pieces Ä‘á»ƒ filter!")
        return [], []
    
    if log_callback:
        log_callback(f"\nğŸ” BÆ¯á»šC 1: Pre-filter vá»›i Bi-encoder cho {len(evidence_pieces)} evidence pieces")
        log_callback(f"   â†’ Sá»­ dá»¥ng Bi-encoder model (nhanh) Ä‘á»ƒ láº¥y top {bi_prefilter_top_n} candidates")
        log_callback(f"   â†’ Claim: {claim}")  # Ghi Ä‘áº§y Ä‘á»§ claim, khÃ´ng truncate
    
    try:
        # BÆ¯á»šC 1: Pre-filter vá»›i Bi-encoder Ä‘á»ƒ giáº£m sá»‘ lÆ°á»£ng evidence cáº§n xá»­ lÃ½ báº±ng CrossEncoder
        # Bi-encoder nhanh hÆ¡n nhiá»u vÃ¬ cÃ³ thá»ƒ encode táº¥t cáº£ cÃ¹ng lÃºc
        bi_scores = compute_evidence_scores_bi_encoder(claim, evidence_pieces)
        
        # Láº¥y top N evidence tá»« Bi-encoder scores
        bi_prefilter_top_n = min(bi_prefilter_top_n, len(evidence_pieces))
        top_bi_indices = np.argsort(-bi_scores)[:bi_prefilter_top_n]
        top_bi_evidence = [evidence_pieces[i] for i in top_bi_indices]
        
        if log_callback:
            log_callback(f"   â†’ ÄÃ£ chá»n top {len(top_bi_evidence)} evidence tá»« Bi-encoder scores")
            log_callback(f"   â†’ Bi-encoder score range: [{bi_scores.min():.4f}, {bi_scores.max():.4f}]")
            log_callback(f"   â†’ Top {min(5, len(top_bi_evidence))} Bi-encoder scores:")
            for idx, ev_idx in enumerate(top_bi_indices[:5]):
                log_callback(f"      [{idx+1}] Score: {bi_scores[ev_idx]:.4f} - {evidence_pieces[ev_idx][:100]}...")
        
        # BÆ¯á»šC 2: TÃ­nh relevance scores báº±ng CrossEncoder chá»‰ cho top N evidence (cháº­m nhÆ°ng chÃ­nh xÃ¡c)
        if log_callback:
            log_callback(f"\nğŸ” BÆ¯á»šC 2: Fine-grained filtering vá»›i CrossEncoder cho {len(top_bi_evidence)} evidence")
            log_callback(f"   â†’ Sá»­ dá»¥ng CrossEncoder model (cháº­m nhÆ°ng chÃ­nh xÃ¡c)")
        
        scores = compute_evidence_scores(claim, top_bi_evidence)
        
        if log_callback:
            log_callback(f"   â†’ Raw scores range: [{scores.min():.4f}, {scores.max():.4f}]")
        
        # Normalize CrossEncoder scores vá» [0, 1] náº¿u cáº§n
        if scores.size > 0:
            # CrossEncoder scores cÃ³ thá»ƒ Ã¢m hoáº·c dÆ°Æ¡ng
            # Strategy: normalize vá» [0, 1] nhÆ°ng giá»¯ nguyÃªn ranking
            min_score = scores.min()
            max_score = scores.max()
            if max_score > min_score:
                cross_scores_normalized = (scores - min_score) / (max_score - min_score)
            else:
                # Táº¥t cáº£ scores báº±ng nhau, set vá» 0.5
                cross_scores_normalized = np.full(len(scores), 0.5)
        else:
            cross_scores_normalized = np.zeros(len(top_bi_evidence))
        
        # Map CrossEncoder scores vá» original evidence_pieces indices
        # cross_scores_normalized[i] corresponds to top_bi_evidence[i] which is evidence_pieces[top_bi_indices[i]]
        scores_map = {}  # original_idx -> normalized_score
        for i in range(len(top_bi_indices)):
            orig_idx = top_bi_indices[i]
            scores_map[orig_idx] = float(cross_scores_normalized[i])
        
        # Create normalized scores array for all evidence pieces
        # Non-selected evidence (not in top_bi_indices) get score 0.0
        scores_normalized = np.zeros(len(evidence_pieces))
        for orig_idx, score in scores_map.items():
            scores_normalized[orig_idx] = score
        
        if log_callback:
            log_callback(f"   â†’ Normalized scores range: [{cross_scores_normalized.min():.4f}, {cross_scores_normalized.max():.4f}]")
            log_callback(f"   â†’ Top 5 evidence scores (CrossEncoder):")
            top_5_cross_indices = np.argsort(-cross_scores_normalized)[:5]
            for idx, cross_idx in enumerate(top_5_cross_indices):
                orig_idx = top_bi_indices[cross_idx]
                log_callback(f"      [{idx+1}] Score: {cross_scores_normalized[cross_idx]:.4f} - {evidence_pieces[orig_idx]}")
        
        # Lá»c evidence cÃ³ relevance > threshold
        # NHÆ¯NG: luÃ´n giá»¯ láº¡i Ã­t nháº¥t top 1 evidence náº¿u cÃ³
        filtered_evidence = []
        filtered_scores = []
        
        # TÃ¬m top evidence indices vÃ  Ä‘iá»u chá»‰nh threshold (dá»±a trÃªn CrossEncoder scores)
        adjusted_threshold = relevance_threshold
        top_cross_indices = list(np.argsort(-cross_scores_normalized))  # Sorted by CrossEncoder scores
        
        if len(top_cross_indices) > 0:
            top_cross_score = cross_scores_normalized[top_cross_indices[0]]
            # Náº¿u top score > 0.5 nhÆ°ng dÆ°á»›i threshold, giáº£m threshold má»™t chÃºt
            if top_cross_score > 0.5 and top_cross_score < relevance_threshold:
                adjusted_threshold = min(relevance_threshold, top_cross_score * 0.8)
                if log_callback:
                    log_callback(f"\nğŸ” BÆ¯á»šC 3: Äiá»u chá»‰nh threshold")
                    log_callback(f"   â†’ Threshold ban Ä‘áº§u: {relevance_threshold}")
                    log_callback(f"   â†’ Top score: {top_cross_score:.4f}")
                    log_callback(f"   â†’ Threshold sau Ä‘iá»u chá»‰nh: {adjusted_threshold:.4f}")
        
        # Map top_cross_indices vá» original indices
        top_indices = [top_bi_indices[cross_idx] for cross_idx in top_cross_indices]
        
        if log_callback:
            log_callback(f"\nğŸ” BÆ¯á»šC 4: Lá»c evidence theo threshold ({adjusted_threshold:.4f})")
        
        # BÆ°á»›c 1: Lá»c evidence theo threshold (chá»‰ xÃ©t cÃ¡c evidence Ä‘Ã£ Ä‘Æ°á»£c pre-filter)
        filtered_indices = set()  # Track indices Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o filtered_evidence
        for orig_idx in top_bi_indices:
            score = scores_map[orig_idx]
            if score >= adjusted_threshold:
                filtered_evidence.append(evidence_pieces[orig_idx])
                filtered_scores.append(score)
                filtered_indices.add(orig_idx)
        
        if log_callback:
            log_callback(f"   â†’ Sá»‘ evidence sau khi lá»c: {len(filtered_evidence)}/{len(top_bi_evidence)} (tá»« {len(evidence_pieces)} ban Ä‘áº§u)")
            if len(filtered_evidence) > 0:
                log_callback(f"   â†’ Evidence Ä‘Æ°á»£c giá»¯ láº¡i:")
                for idx, (ev, score) in enumerate(zip(filtered_evidence, filtered_scores)):
                    # Ghi Ä‘áº§y Ä‘á»§ evidence, khÃ´ng truncate
                    log_callback(f"      [{idx+1}] Score: {score:.4f} - {ev}")
        
        # BÆ°á»›c 2: Náº¿u sá»‘ lÆ°á»£ng evidence sau khi lá»c < min_keep, bá»• sung top evidence
        # Äáº£m báº£o luÃ´n cÃ³ Ã­t nháº¥t min_keep evidence (hoáº·c táº¥t cáº£ náº¿u Ã­t hÆ¡n min_keep)
        if len(filtered_evidence) < min_keep and len(top_indices) > 0:
            if log_callback:
                log_callback(f"\nğŸ” BÆ¯á»šC 5: Bá»• sung evidence Ä‘á»ƒ Ä‘áº¡t min_keep={min_keep}")
                log_callback(f"   â†’ Hiá»‡n táº¡i cÃ³ {len(filtered_evidence)} evidence, cáº§n thÃªm {min_keep - len(filtered_evidence)}")
            
            # ThÃªm top evidence chÆ°a cÃ³ trong filtered_evidence
            added_count = 0
            for orig_idx in top_indices:
                if len(filtered_evidence) >= min_keep:
                    break
                if orig_idx not in filtered_indices:
                    score = scores_map[orig_idx]
                    # Chá»‰ thÃªm náº¿u score > 0.2 (ngÆ°á»¡ng tá»‘i thiá»ƒu)
                    if score > 0.2:
                        filtered_evidence.append(evidence_pieces[orig_idx])
                        filtered_scores.append(score)
                        filtered_indices.add(orig_idx)
                        added_count += 1
                        if log_callback:
                            # Ghi Ä‘áº§y Ä‘á»§ evidence, khÃ´ng truncate
                            log_callback(f"      [+] ThÃªm evidence #{orig_idx} (score: {score:.4f}) - {evidence_pieces[orig_idx]}")
            
            if log_callback:
                log_callback(f"   â†’ ÄÃ£ thÃªm {added_count} evidence")
        
        # BÆ°á»›c 3: Sáº¯p xáº¿p láº¡i theo score (descending) Ä‘á»ƒ Ä‘áº£m báº£o top evidence á»Ÿ Ä‘áº§u
        if filtered_evidence and len(filtered_evidence) > 1:
            if log_callback:
                log_callback(f"\nğŸ” BÆ¯á»šC 6: Sáº¯p xáº¿p láº¡i evidence theo score (descending)")
            
            # Táº¡o list of tuples (score, evidence) Ä‘á»ƒ sort
            evidence_score_pairs = list(zip(filtered_scores, filtered_evidence))
            evidence_score_pairs.sort(reverse=True, key=lambda x: x[0])
            filtered_evidence = [ev for _, ev in evidence_score_pairs]
            filtered_scores = [score for score, _ in evidence_score_pairs]
        
        if log_callback:
            log_callback(f"\nâœ… Káº¾T QUáº¢: ÄÃ£ chá»n {len(filtered_evidence)} evidence tá»« {len(evidence_pieces)} evidence ban Ä‘áº§u")
            log_callback(f"   â†’ ÄÃ£ pre-filter {len(top_bi_evidence)} evidence báº±ng Bi-encoder, sau Ä‘Ã³ dÃ¹ng CrossEncoder")
            if len(filtered_evidence) > 0:
                log_callback(f"   â†’ Score range: [{min(filtered_scores):.4f}, {max(filtered_scores):.4f}]")
        
        return filtered_evidence, filtered_scores
    except Exception as e:
        if log_callback:
            log_callback(f"âŒ Lá»–I khi filter evidence: {e}")
        print(f"Error filtering evidence by relevance: {e}")
        # Náº¿u lá»—i, tráº£ vá» toÃ n bá»™ evidence (khÃ´ng filter)
        return evidence_pieces, [0.5] * len(evidence_pieces)


def _llm_judge_with_evidence(claim: str, evidence_pieces: List[str], top_k: int = 5, log_callback=None) -> tuple:
    """
    DÃ¹ng LLM (Ollama) Ä‘á»ƒ ra phÃ¡n quyáº¿t dá»±a trÃªn claim + cÃ¡c báº±ng chá»©ng web.
    Giáº£m tá»‘i Ä‘a rule-based; LLM chá»‹u trÃ¡ch nhiá»‡m phÃ¢n loáº¡i NLI.
    
    BÆ¯á»šC 1: Lá»c evidence khÃ´ng liÃªn quan trÆ°á»›c khi judge.
    BÆ¯á»šC 2: Chá»n top_k evidence liÃªn quan nháº¥t.
    BÆ¯á»šC 3: LLM judge vá»›i prompt yÃªu cáº§u kiá»ƒm tra relevance.
    
    Args:
        claim: Claim cáº§n fact-check
        evidence_pieces: Danh sÃ¡ch evidence pieces
        top_k: Sá»‘ lÆ°á»£ng evidence tá»‘i Ä‘a Ä‘á»ƒ Ä‘Æ°a vÃ o judge
        log_callback: HÃ m callback Ä‘á»ƒ log cÃ¡c bÆ°á»›c (optional)
    
    Returns:
        tuple: (verdict_string, evidence_info_dict)
        - verdict_string: String chá»©a verdict vÃ  justification
        - evidence_info_dict: Dict chá»©a thÃ´ng tin vá» evidence (selected_evidence, selected_scores, stats)
    """
    if log_callback:
        log_callback(f"\n{'='*80}")
        log_callback(f"ğŸ” QUÃ TRÃŒNH Lá»ŒC VÃ€ CHá»ŒN EVIDENCE CHO JUDGE")
        log_callback(f"{'='*80}")
        log_callback(f"Claim: {claim}")
        log_callback(f"Tá»•ng sá»‘ evidence ban Ä‘áº§u: {len(evidence_pieces)}")
        log_callback(f"Top_k: {top_k}")
    
    # BÆ¯á»šC 1: Lá»c evidence khÃ´ng liÃªn quan (relevance threshold = 0.15, giáº£m Ä‘á»ƒ giá»¯ nhiá»u evidence hÆ¡n)
    # Äáº£m báº£o giá»¯ láº¡i Ã­t nháº¥t top_k evidence (hoáº·c táº¥t cáº£ náº¿u Ã­t hÆ¡n top_k)
    filtered_evidence, relevance_scores = filter_evidence_by_relevance(
        claim, evidence_pieces, relevance_threshold=0.15, min_keep=top_k, log_callback=log_callback
    )
    
    # Náº¿u khÃ´ng cÃ³ evidence nÃ o liÃªn quan, tráº£ vá» Not Enough Evidence ngay
    if not filtered_evidence:
        justification = (
            f"KhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng nÃ o liÃªn quan Ä‘áº¿n claim. "
            f"ÄÃ£ kiá»ƒm tra {len(evidence_pieces)} báº±ng chá»©ng nhÆ°ng táº¥t cáº£ Ä‘á»u cÃ³ Ä‘á»™ liÃªn quan tháº¥p."
        )
        verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`Not Enough Evidence`"
        evidence_info = {
            "claim": claim,
            "total_evidence": len(evidence_pieces),
            "filtered_evidence_count": 0,
            "selected_evidence_count": 0,
            "top_k": top_k,
            "selected_evidence": [],
            "selected_scores": []
        }
        return verdict_string, evidence_info
    
    # BÆ¯á»šC 2: Chá»n top_k evidence liÃªn quan nháº¥t tá»« filtered_evidence
    if log_callback:
        log_callback(f"\nğŸ” BÆ¯á»šC 6: Chá»n top_{top_k} evidence tá»« {len(filtered_evidence)} evidence Ä‘Ã£ lá»c")
    
    try:
        # TÃ­nh láº¡i scores cho filtered evidence Ä‘á»ƒ rank chÃ­nh xÃ¡c
        scores = compute_evidence_scores(claim, filtered_evidence)
        if scores.size == 0:
            ranked_indices = list(range(len(filtered_evidence)))
        else:
            ranked_indices = list(np.argsort(-scores))
    except Exception:
        ranked_indices = list(range(len(filtered_evidence)))

    top_k = min(top_k, len(ranked_indices))
    selected_idx = ranked_indices[:top_k]
    selected_evidence = [filtered_evidence[i] for i in selected_idx]
    selected_scores = [relevance_scores[i] for i in selected_idx]
    
    if log_callback:
        log_callback(f"   â†’ ÄÃ£ chá»n {len(selected_evidence)} evidence:")
        for i, (ev, score) in enumerate(zip(selected_evidence, selected_scores)):
            # Ghi Ä‘áº§y Ä‘á»§ evidence, khÃ´ng truncate
            log_callback(f"      [E{i}] Score: {score:.4f} - {ev}")

    # Táº¡o evidence_info dict Ä‘á»ƒ tráº£ vá»
    evidence_info = {
        "claim": claim,
        "total_evidence": len(evidence_pieces),
        "filtered_evidence_count": len(filtered_evidence),
        "selected_evidence_count": len(selected_evidence),
        "top_k": top_k,
        "selected_evidence": selected_evidence,
        "selected_scores": selected_scores
    }
    
    # In ra danh sÃ¡ch báº±ng chá»©ng trÆ°á»›c khi Ä‘Æ°a vÃ o judge
    print("\n" + "=" * 80)
    print("ğŸ“‹ DANH SÃCH Báº°NG CHá»¨NG ÄÆ¯á»¢C CHá»ŒN CHO JUDGE:")
    print("=" * 80)
    print(f"Claim: {claim}")
    print(f"\nTá»•ng sá»‘ báº±ng chá»©ng ban Ä‘áº§u: {len(evidence_pieces)}")
    print(f"Sá»‘ báº±ng chá»©ng sau khi lá»c (relevance > 0.15): {len(filtered_evidence)}")
    print(f"Sá»‘ báº±ng chá»©ng Ä‘Æ°á»£c chá»n (top_k={top_k}): {len(selected_evidence)}")
    print("\n" + "-" * 80)
    for i, (ev, score) in enumerate(zip(selected_evidence, selected_scores)):
        print(f"\n[E{i}] (Relevance score: {score:.4f})")
        ev_preview = ev
        print(f"{ev_preview}")
    print("\n" + "=" * 80 + "\n")

    # XÃ¢y prompt cho LLM (tiáº¿ng Viá»‡t, output JSON)
    evidence_block_lines = []
    for i, ev in enumerate(selected_evidence):
        evidence_block_lines.append(f"- [E{i}] {ev}")
    evidence_block = "\n".join(evidence_block_lines)

    prompt = f"""PhÃ¢n loáº¡i YÃŠU Cáº¦U dá»±a trÃªn Báº°NG CHá»¨NG thÃ nh 1 trong NHÃƒN. Tráº£ vá» JSON.

NHÃƒN:
Supported
- DÃ¹ng khi cÃ³ báº±ng chá»©ng E[i] rÃµ rÃ ng, trá»±c tiáº¿p á»¦NG Há»˜ yÃªu cáº§u.
- Náº¿u yÃªu cáº§u cÃ³ nhiá»u khÃ­a cáº¡nh, Táº¤T Cáº¢ cÃ¡c khÃ­a cáº¡nh Cá»T LÃ•I pháº£i Ä‘Æ°á»£c á»¦NG Há»˜ Ä‘á»ƒ chá»n phÃ¡n quyáº¿t nÃ y.
- LÆ¯U Ã: Chá»‰ cáº§n báº±ng chá»©ng há»— trá»£ Cá»T LÃ•I cá»§a yÃªu cáº§u, khÃ´ng cáº§n pháº£i khá»›p 100% tá»«ng tá»«.
- LÆ¯U Ã: Náº¿u báº±ng chá»©ng xÃ¡c nháº­n Ã NGHÄ¨A cá»§a yÃªu cáº§u (dÃ¹ dÃ¹ng tá»« khÃ¡c) thÃ¬ váº«n coi lÃ  Supported.
- Vá»€ THá»œI GIAN/TIáº¾N TRÃŒNH: 
  * Náº¿u yÃªu cáº§u nÃ³i "Ä‘Ã£ Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh" nhÆ°ng báº±ng chá»©ng nÃ³i "Ä‘Ã£ Ä‘á» nghá»‹/Ä‘Ã£ tá» trÃ¬nh", hÃ£y kiá»ƒm tra ká»¹:
    - Náº¿u báº±ng chá»©ng CÅ¨NG nÃ³i vá» "Quyáº¿t Ä‘á»‹nh sá»‘...", "Ä‘Æ°á»£c bá»• sung vÃ o...", "xáº¿p háº¡ng...", "cÃ´ng nháº­n..." â†’ coi lÃ  Ä‘Ã£ Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh â†’ Supported
    - Náº¿u báº±ng chá»©ng CHá»ˆ nÃ³i "tá» trÃ¬nh Ä‘á» nghá»‹" mÃ  KHÃ”NG cÃ³ thÃ´ng tin vá» quyáº¿t Ä‘á»‹nh sau Ä‘Ã³ â†’ Not Enough Evidence
  * Quy trÃ¬nh: "Ä‘á» nghá»‹" â†’ "thÃ´ng qua" â†’ "quyáº¿t Ä‘á»‹nh" lÃ  bÃ¬nh thÆ°á»ng. Náº¿u báº±ng chá»©ng nÃ³i vá» quyáº¿t Ä‘á»‹nh hoáº·c káº¿t quáº£ cuá»‘i cÃ¹ng, coi lÃ  Supported.

Refuted
- DÃ¹ng khi cÃ³ báº±ng chá»©ng E[i] rÃµ rÃ ng BÃC Bá» hoáº·c MÃ‚U THUáºªN trá»±c tiáº¿p vá»›i yÃªu cáº§u.
- VÃ­ dá»¥: YÃªu cáº§u nÃ³i "A lÃ  B" nhÆ°ng báº±ng chá»©ng nÃ³i "A khÃ´ng pháº£i lÃ  B" hoáº·c "A lÃ  C" (khÃ´ng pháº£i B).
- Náº¿u yÃªu cáº§u cÃ³ nhiá»u khÃ­a cáº¡nh, dÃ¹ chá»‰ 1 khÃ­a cáº¡nh Cá»T LÃ•I bá»‹ BÃC Bá» thÃ¬ cÅ©ng Ä‘á»§ Ä‘á»ƒ chá»n phÃ¡n quyáº¿t nÃ y.
- LÆ¯U Ã: KHÃ”NG chá»n Refuted náº¿u báº±ng chá»©ng chá»‰ KHÃ”NG NHáº®C Äáº¾N má»™t sá»‘ chi tiáº¿t phá»¥ trong yÃªu cáº§u. Chá»‰ chá»n khi cÃ³ mÃ¢u thuáº«n trá»±c tiáº¿p.
- LÆ¯U Ã: KHÃ”NG chá»n Refuted chá»‰ vÃ¬ báº±ng chá»©ng dÃ¹ng tá»« khÃ¡c nhÆ°ng Ã½ nghÄ©a giá»‘ng nhau.

Not Enough Evidence
- DÃ¹ng khi táº¥t cáº£ E[i] KHÃ”NG Äá»¦ thÃ´ng tin Ä‘á»ƒ xÃ¡c nháº­n hoáº·c bÃ¡c bá» yÃªu cáº§u.
- DÃ¹ng khi báº±ng chá»©ng khÃ´ng liÃªn quan hoáº·c quÃ¡ mÆ¡ há»“.
- DÃ¹ng náº¿u yÃªu cáº§u quÃ¡ MÆ  Há»’ hoáº·c khÃ´ng thá»ƒ kiá»ƒm chá»©ng báº±ng dá»¯ liá»‡u hiá»‡n cÃ³.
- LÆ¯U Ã: KHÃ”NG chá»n Not Enough Evidence náº¿u báº±ng chá»©ng Ä‘Ã£ há»— trá»£ Cá»T LÃ•I cá»§a yÃªu cáº§u, chá»‰ thiáº¿u má»™t sá»‘ chi tiáº¿t phá»¥.

QUAN TRá»ŒNG:
1. Táº­p trung vÃ o Cá»T LÃ•I cá»§a yÃªu cáº§u, khÃ´ng yÃªu cáº§u khá»›p 100% tá»«ng tá»«.
2. Hiá»ƒu Ã NGHÄ¨A cá»§a yÃªu cáº§u, khÃ´ng chá»‰ tÃ¬m cá»¥m tá»« chÃ­nh xÃ¡c.
3. Vá» THá»œI GIAN/TIáº¾N TRÃŒNH: 
   - Náº¿u yÃªu cáº§u nÃ³i "Ä‘Ã£ Ä‘Æ°á»£c X" vÃ  báº±ng chá»©ng nÃ³i vá» quyáº¿t Ä‘á»‹nh/káº¿t quáº£ liÃªn quan Ä‘áº¿n X â†’ Supported
   - Náº¿u báº±ng chá»©ng nÃ³i vá» cáº£ "Ä‘á» nghá»‹" VÃ€ "quyáº¿t Ä‘á»‹nh/káº¿t quáº£" â†’ Supported (quyáº¿t Ä‘á»‹nh lÃ  káº¿t quáº£ cuá»‘i)
   - Náº¿u báº±ng chá»©ng CHá»ˆ nÃ³i vá» "Ä‘á» nghá»‹" mÃ  khÃ´ng cÃ³ thÃ´ng tin vá» káº¿t quáº£ â†’ Not Enough Evidence
4. Náº¿u báº±ng chá»©ng há»— trá»£ pháº§n lá»›n yÃªu cáº§u, chá»‰ thiáº¿u má»™t sá»‘ chi tiáº¿t phá»¥, váº«n chá»n Supported.
5. Äá»c Ká»¸ toÃ n bá»™ báº±ng chá»©ng: má»™t báº±ng chá»©ng cÃ³ thá»ƒ chá»©a cáº£ "Ä‘á» nghá»‹" vÃ  "quyáº¿t Ä‘á»‹nh" á»Ÿ cÃ¡c pháº§n khÃ¡c nhau.

Äá»ŠNH Dáº NG (báº¯t buá»™c JSON, khÃ´ng cÃ³ text khÃ¡c):
{{
  "verdict": "Supported|Refuted|Not Enough Evidence",
  "justification": "Giáº£i thÃ­ch ngáº¯n gá»n (1-2 cÃ¢u), nÃªu rÃµ [Ei] nÃ o Ä‘Æ°á»£c dÃ¹ng vÃ  lÃ½ do chá»n nhÃ£n nÃ y."
}}

YÃŠU Cáº¦U:
{claim}

Báº°NG CHá»¨NG:
{evidence_block}

JSON:
"""
    try:
        # Kiá»ƒm tra judge provider tá»« env var
        judge_provider = os.getenv("FACTCHECKER_JUDGE_PROVIDER", "ollama").lower()
        
        if judge_provider == "gemini":
            # DÃ¹ng Gemini API
            gemini_model = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
            raw = llm.prompt_gemini(prompt, model=gemini_model)
        else:
            # Máº·c Ä‘á»‹nh dÃ¹ng Ollama
            raw = llm.prompt_ollama(prompt, think=False, use_judge_model=True)
    except Exception as e:
        # Náº¿u LLM lá»—i, fallback an toÃ n
        justification = f"Lá»—i khi gá»i LLM judge: {e}.Initial Action Execution:Initial Action Execution: Máº·c Ä‘á»‹nh Not Enough Evidence."
        verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`Not Enough Evidence`"
        return verdict_string, evidence_info

    # Cáº£i thiá»‡n JSON parsing vá»›i nhiá»u strategies
    if not raw or not raw.strip():
        justification = "LLM judge khÃ´ng tráº£ vá» káº¿t quáº£. Máº·c Ä‘á»‹nh Not Enough Evidence."
        verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`Not Enough Evidence`"
        return verdict_string, evidence_info
    
    # Strategy 1: TÃ¬m JSON block trong markdown code block
    import re
    json_in_code_block = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', raw, re.DOTALL)
    if json_in_code_block:
        json_candidate = json_in_code_block.group(1).strip()
        try:
            obj = json.loads(json_candidate)
            verdict_raw = obj.get("verdict", "")
            justification = obj.get("justification", "").strip()
            verdict = _normalize_llm_verdict(verdict_raw)
            if justification:
                verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`{verdict}`"
                return verdict_string, evidence_info
        except Exception:
            pass
    
    # Strategy 2: TÃ¬m JSON object trong text (tá»« '{' Ä‘áº§u tiÃªn Ä‘áº¿n '}' cuá»‘i cÃ¹ng matching)
    # TÃ¬m táº¥t cáº£ cÃ¡c cáº·p {} vÃ  thá»­ parse
    brace_pattern = re.search(r'\{[^{}]*"verdict"[^{}]*\}', raw, re.DOTALL)
    if brace_pattern:
        json_candidate = brace_pattern.group(0)
        try:
            obj = json.loads(json_candidate)
            verdict_raw = obj.get("verdict", "")
            justification = obj.get("justification", "").strip()
            verdict = _normalize_llm_verdict(verdict_raw)
            if justification:
                verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`{verdict}`"
                return verdict_string, evidence_info
        except Exception:
            pass
    
    # Strategy 3: TÃ¬m tá»« '{' Ä‘áº§u tiÃªn Ä‘áº¿n '}' cuá»‘i cÃ¹ng (original method)
    start = raw.find("{")
    end = raw.rfind("}")
    if start != -1 and end != -1 and end > start:
        json_candidate = raw[start:end + 1]
        try:
            obj = json.loads(json_candidate)
            verdict_raw = obj.get("verdict", "")
            justification = obj.get("justification", "").strip()
            verdict = _normalize_llm_verdict(verdict_raw)
            if justification:
                verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`{verdict}`"
                return verdict_string, evidence_info
        except Exception as e:
            pass
    
    # Strategy 4: Extract verdict tá»« text náº¿u khÃ´ng parse Ä‘Æ°á»£c JSON
    # TÃ¬m cÃ¡c tá»« khÃ³a verdict trong text
    raw_lower = raw.lower()
    if '"supported"' in raw_lower or "supported" in raw_lower:
        verdict = "Supported"
    elif '"refuted"' in raw_lower or "refuted" in raw_lower:
        verdict = "Refuted"
    else:
        verdict = "Not Enough Evidence"
    
    # Fallback: normalize tá»« raw text
    verdict = _normalize_llm_verdict(raw)
    justification = f"KhÃ´ng parse Ä‘Æ°á»£c JSON tá»« output LLM. Dá»±a trÃªn ná»™i dung: {raw[:200]}... Chá»n nhÃ£n {verdict}."
    verdict_string = f"### Justification:\n{justification}\n\n### Verdict:\n`{verdict}`"
    return verdict_string, evidence_info


def judge(record):
    """
    PhiÃªn báº£n judge má»›i:
    - Váº«n dÃ¹ng retrieval tá»« web (evidence Ä‘Ã£ Ä‘Æ°á»£c thu tháº­p á»Ÿ bÆ°á»›c trÆ°á»›c).
    - Bá» pháº§n rule-based phá»©c táº¡p cho verdict cuá»‘i cÃ¹ng.
    - DÃ¹ng LLM (Ollama) Ä‘á»ƒ phÃ¢n loáº¡i dá»±a trÃªn claim + cÃ¡c evidence quan trá»ng nháº¥t.
    """
    # Extract claim and evidence tá»« record (report.md)
    claim = extract_claim_from_record(record)
    evidence_pieces = extract_evidence_pieces(record)

    if not claim:
        verdict_string = "### Justification:\nKhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh yÃªu cáº§u tá»« báº£n ghi.\n\n### Verdict:\n`Not Enough Evidence`"
        evidence_info = {
            "claim": "",
            "total_evidence": 0,
            "filtered_evidence_count": 0,
            "selected_evidence_count": 0,
            "top_k": 6,
            "selected_evidence": [],
            "selected_scores": []
        }
        return verdict_string, evidence_info

    if not evidence_pieces:
        verdict_string = "### Justification:\nKhÃ´ng tÃ¬m tháº¥y báº±ng chá»©ng nÃ o trong báº£n ghi.\n\n### Verdict:\n`Not Enough Evidence`"
        evidence_info = {
            "claim": claim,
            "total_evidence": 0,
            "filtered_evidence_count": 0,
            "selected_evidence_count": 0,
            "top_k": 6,
            "selected_evidence": [],
            "selected_scores": []
        }
        return verdict_string, evidence_info

    # Gá»i LLM Ä‘á»ƒ judge dá»±a trÃªn claim + evidence (Ä‘Ã£ chá»n top báº±ng CrossEncoder)
    # Táº¡o log callback Ä‘á»ƒ ghi láº¡i quÃ¡ trÃ¬nh filter vÃ  select evidence
    filter_log_lines = []
    def filter_log_callback(msg):
        filter_log_lines.append(msg)
        print(f"[EVIDENCE_FILTER] {msg}")
    
    # DÃ¹ng top_k=2 Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ judge (giáº£m tá»« 3 xuá»‘ng 2)
    verdict_string, evidence_info = _llm_judge_with_evidence(claim, evidence_pieces, top_k=3, log_callback=filter_log_callback)
    
    # Ghi log vÃ o evidence_info Ä‘á»ƒ cÃ³ thá»ƒ append vÃ o report sau
    if filter_log_lines:
        evidence_info['filter_log'] = filter_log_lines
    
    return verdict_string, evidence_info

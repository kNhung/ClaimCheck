from .llm import prompt_ollama

judge_prompt = """
Instructions
Determine the Claim's veracity by following these steps:
1. Briefly summarize the key insights from the fact-check (see Record) in at most one paragraph.
2. Write one paragraph about which one of the Decision Options applies best. Include the most appropriate decision option at the end and enclose it in backticks
like `this`.

Decision Options:
{options}

Rules:
{rules}

Record:
{record}

Important: Write your final answer in Vietnamese.

Your Judgement:
"""

verdict_extraction_prompt = """
Instructions
You are a fact-checker, and your task is to extract the verdict from the fact-check conclusion. Only return one verdict. The verdict should be one of the following options:

Decision Options:
{options}

Rules:
{rules}

Conclusion:
{conclusion}


Extracted Verdict:
"""

def judge(record, decision_options, rules="", think=True):
    """
    Determines the veracity of a claim based on the provided record and decision options.

    Args:
        record (str): The record containing evidence for the judgement.
        decision_options (str): The available decision options to choose from.
        extra_rules (str): Additional rules to consider in the judgement.
        think (bool): Whether to use chain-of-thought (default True)
    Returns:
        str: The judgement generated by the LLM.
    """
    prompt = judge_prompt.format(record=record, options=decision_options, rules=rules)
    return prompt_ollama(prompt, think=think)

def extract_verdict(conclusion, decision_options, rules=""):
    """
    Extracts the verdict from the conclusion of a fact-check.

    Args:
        conclusion (str): The conclusion text from which to extract the verdict.
        decision_options (str): The available decision options to choose from.
        rules (str): Additional rules to consider in the extraction.
    
    Returns:
        str: The extracted verdict.
    """
    prompt = verdict_extraction_prompt.format(conclusion=conclusion, options=decision_options, rules=rules)
    return prompt_ollama(prompt, think=False)